{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be58009e-8307-40f2-9d0f-d1158c50ed8f",
   "metadata": {},
   "source": [
    "# Install the necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1e35dea-a981-414b-901b-a298f254f2bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T13:46:35.390397Z",
     "iopub.status.busy": "2023-11-16T13:46:35.389864Z",
     "iopub.status.idle": "2023-11-16T13:46:38.343339Z",
     "shell.execute_reply": "2023-11-16T13:46:38.342508Z",
     "shell.execute_reply.started": "2023-11-16T13:46:35.390394Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#install the dependencies\n",
    "!pip install -qU transformers accelerate einops xformers bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18ddc65-c88c-4b6a-bce7-402fefd57f66",
   "metadata": {},
   "source": [
    "# Login to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fe2dab0-e739-44bd-83ea-66a499abd85e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T11:07:44.817420Z",
     "iopub.status.busy": "2023-11-15T11:07:44.816789Z",
     "iopub.status.idle": "2023-11-15T11:07:44.835911Z",
     "shell.execute_reply": "2023-11-15T11:07:44.835206Z",
     "shell.execute_reply.started": "2023-11-15T11:07:44.817390Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a67b4e44d2f1484bada79450e101ca46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from huggingface_hub import login\n",
    "# login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f100fd-925b-4269-a7b3-ea03f86219a1",
   "metadata": {},
   "source": [
    "# Initialize and load a pre-trained model for language modeling using the Hugging Face Transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f99faa6e-62d6-4f3e-b90c-d03bc1058985",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T13:46:48.195106Z",
     "iopub.status.busy": "2023-11-16T13:46:48.194540Z",
     "iopub.status.idle": "2023-11-16T14:26:14.791004Z",
     "shell.execute_reply": "2023-11-16T14:26:14.790363Z",
     "shell.execute_reply.started": "2023-11-16T13:46:48.195077Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d395025183d410199b1f744e95f71b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)iuae/falcon-40b/resolve/main/config.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c0465478fca4c8c830a798dd8d4a8e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)40b/resolve/main/configuration_falcon.py:   0%|          | 0.00/7.16k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-40b:\n",
      "- configuration_falcon.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "\n",
      "WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "504609d4575b4d7cb6e8e965c4677f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)lcon-40b/resolve/main/modeling_falcon.py:   0%|          | 0.00/56.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-40b:\n",
      "- modeling_falcon.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0069cd9f241140de97ccfb23a965c63b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)esolve/main/pytorch_model.bin.index.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db4973c812e94635b9b0e55c0071861e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93ad239346bf446badb105c38629bc25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00009.bin:   0%|          | 0.00/9.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb8f9ccb59ef4906b8f215d6675255f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00009.bin:   0%|          | 0.00/9.51G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25d3db8f46e84626b9579ba432a998cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00003-of-00009.bin:   0%|          | 0.00/9.51G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2506e7b647ef467c90be45ebbb0db2d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00004-of-00009.bin:   0%|          | 0.00/9.51G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "313b3f2494984b0ab2289d0d7b788568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00005-of-00009.bin:   0%|          | 0.00/9.51G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18ca5c855a3a42b29879bfe234829141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00006-of-00009.bin:   0%|          | 0.00/9.51G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3401e7392fb9490ab235a9f5d2f73cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00007-of-00009.bin:   0%|          | 0.00/9.51G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f620fce3840a4196bebd869130278a1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00008-of-00009.bin:   0%|          | 0.00/9.51G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b72b11892a0145478679b7f92eb4df5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00009-of-00009.bin:   0%|          | 0.00/7.58G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0091336e3ff4b118c35979954278c4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60c42ddd450d46fbaf3c5bb2367b5cb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-40b/resolve/main/generation_config.json:   0%|          | 0.00/117 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "#import necessary libraries to get the model running\n",
    "import torch\n",
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "\n",
    "model_name = 'tiiuae/falcon-40b'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# set quantization configuration to load large model with less GPU memory\n",
    "# this requires the `bitsandbytes` library\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto'\n",
    ")\n",
    "model.eval()\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8558c7-2c12-4902-a58e-77a53ddc237d",
   "metadata": {},
   "source": [
    "# Create the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8520da26-b608-4909-9980-2a9c5e896b29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T14:27:19.229099Z",
     "iopub.status.busy": "2023-11-16T14:27:19.228569Z",
     "iopub.status.idle": "2023-11-16T14:27:19.677464Z",
     "shell.execute_reply": "2023-11-16T14:27:19.676819Z",
     "shell.execute_reply.started": "2023-11-16T14:27:19.229073Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a4d3cbe0fb74a2eae89af1d0a606786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)n-40b/resolve/main/tokenizer_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9721cde9ea99437e97bf9041b85ecc3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)e/falcon-40b/resolve/main/tokenizer.json:   0%|          | 0.00/2.73M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a1be9b6663a49d382afed8a6dd0de39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)40b/resolve/main/special_tokens_map.json:   0%|          | 0.00/281 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#create a tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267096a1-c7c3-41a1-ab65-ba683055e0be",
   "metadata": {},
   "source": [
    "# Convert specific token sequences into their numerical representations a.k.a (token IDs) using a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fe992fe-8d9c-4c27-a01f-cb0fcfab2e82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T14:27:24.684324Z",
     "iopub.status.busy": "2023-11-16T14:27:24.684037Z",
     "iopub.status.idle": "2023-11-16T14:27:24.689529Z",
     "shell.execute_reply": "2023-11-16T14:27:24.688993Z",
     "shell.execute_reply.started": "2023-11-16T14:27:24.684303Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[23431, 37], [17362, 37]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "#convert token sequences into their corresponding token IDs\n",
    "stop_token_ids = [tokenizer.convert_tokens_to_ids(x) for x in [['Human', ':'], ['AI', ':']]]\n",
    "stop_token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b05cd1-b2a0-4918-a0e7-ed74305caf46",
   "metadata": {},
   "source": [
    "# Convert each integer value in the stop_token_ids list into a PyTorch LongTensor and then move the resulting tensors to a specified device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b471c840-030c-4645-84b9-3cccd3860e63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T14:27:30.326249Z",
     "iopub.status.busy": "2023-11-16T14:27:30.325424Z",
     "iopub.status.idle": "2023-11-16T14:27:30.596163Z",
     "shell.execute_reply": "2023-11-16T14:27:30.595261Z",
     "shell.execute_reply.started": "2023-11-16T14:27:30.326225Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([23431,    37], device='cuda:0'),\n",
       " tensor([17362,    37], device='cuda:0')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "#converting each integer value in stop_token_ids into a PyTorch LongTensor and moving it to a specified device\n",
    "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
    "stop_token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e41f5f-8f80-4acb-847f-d7822e33e3bb",
   "metadata": {},
   "source": [
    "# Define the Stopping criteria for Falcon 40b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca9b813f-cdc5-4b10-8576-cf01912d4ec9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T14:27:35.124737Z",
     "iopub.status.busy": "2023-11-16T14:27:35.123948Z",
     "iopub.status.idle": "2023-11-16T14:27:35.129365Z",
     "shell.execute_reply": "2023-11-16T14:27:35.128674Z",
     "shell.execute_reply.started": "2023-11-16T14:27:35.124702Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "# a custom stopping criteria object\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_ids in stop_token_ids:\n",
    "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d258000e-30c1-4fc2-b123-182303489ca0",
   "metadata": {},
   "source": [
    "# Set up a the text generation pipeline using the Hugging Face Transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d83737e-b10b-46dc-adb1-f89f640f3f52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T14:27:39.391261Z",
     "iopub.status.busy": "2023-11-16T14:27:39.390632Z",
     "iopub.status.idle": "2023-11-16T14:27:41.055774Z",
     "shell.execute_reply": "2023-11-16T14:27:41.055227Z",
     "shell.execute_reply.started": "2023-11-16T14:27:39.391236Z"
    }
   },
   "outputs": [],
   "source": [
    "#create the model pipeline\n",
    "generate_text = transformers.pipeline(\n",
    "    model=model, #pass the model\n",
    "    tokenizer=tokenizer, #pass the tokenizer\n",
    "    return_full_text=True,  #to return the original query, making it easier for prompting.\n",
    "    task='text-generation', #task\n",
    "    # we pass model parameters here too\n",
    "    stopping_criteria=stopping_criteria,  #to eliminate unnecessary conversations\n",
    "    temperature=0.3,  #for 'randomness' of model outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens=512,  #max number of tokens to generate in the output\n",
    "    repetition_penalty=1.1  #without this output begins repeating (make sure to experiment with this)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d96bbc5-b0ac-49ae-88f4-48c80f5f6bc6",
   "metadata": {},
   "source": [
    "# Generate the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de207d65-5d10-4c05-a173-839e3c2a4d40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T14:27:46.611192Z",
     "iopub.status.busy": "2023-11-16T14:27:46.610199Z",
     "iopub.status.idle": "2023-11-16T14:28:02.590208Z",
     "shell.execute_reply": "2023-11-16T14:28:02.589549Z",
     "shell.execute_reply.started": "2023-11-16T14:27:46.611147Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain to me the difference between centrifugal force and centripetal force.\n",
      "Centrifugal force is a fictitious force that acts on an object moving in a circular path. It is directed away from the center of rotation. Centripetal force is the real force that acts on an object moving in a circular path. It is directed toward the center of rotation.\n",
      "This page was last updated June 27, 2015.\n"
     ]
    }
   ],
   "source": [
    "#generate output\n",
    "res = generate_text(\"Explain to me the difference between centrifugal force and centripetal force.\")\n",
    "print(res[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c26515ed-0ffe-449d-9104-a4c24d775522",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T14:28:10.502133Z",
     "iopub.status.busy": "2023-11-16T14:28:10.501576Z",
     "iopub.status.idle": "2023-11-16T14:29:43.648088Z",
     "shell.execute_reply": "2023-11-16T14:29:43.647528Z",
     "shell.execute_reply.started": "2023-11-16T14:28:10.502111Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    }
   ],
   "source": [
    "#generate sequential output\n",
    "sequences = generate_text(\"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n",
    "    max_length=200,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93313606-b397-41e9-8ad0-f1f0a4920feb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T14:29:53.483543Z",
     "iopub.status.busy": "2023-11-16T14:29:53.482759Z",
     "iopub.status.idle": "2023-11-16T14:29:53.486472Z",
     "shell.execute_reply": "2023-11-16T14:29:53.486037Z",
     "shell.execute_reply.started": "2023-11-16T14:29:53.483519Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\n",
      "Daniel: Hello, Girafatron!\n",
      "Girafatron: Hello, Daniel.\n",
      "Daniel: How are you today?\n",
      "Girafatron: I am fine, thank you.\n",
      "Daniel: What is your favorite food?\n",
      "Girafatron: I enjoy eating leaves from the tallest trees.\n",
      "Daniel: What is your favorite color?\n",
      "Girafatron: I like the color yellow.\n",
      "Daniel: What is your favorite movie?\n",
      "Girafatron: I like the movie \"Giraffes Gone Wild.\"\n",
      "Daniel: What is your favorite television show?\n",
      "Girafatron: I like the show \"Giraffes Gone Wild.\"\n",
      "Daniel: What is your favorite book?\n",
      "Girafatron: I like the book \"Giraffes Gone Wild.\"\n",
      "Daniel: What is your favorite song?\n",
      "Girafatron: I like the song \"Giraffes Gone Wild.\"\n",
      "Daniel: What is your favorite band?\n",
      "Girafatron: I like the band \"Giraffes Gone Wild.\"\n",
      "Daniel: What is your favorite sport?\n",
      "Girafatron: I like the sport \"Giraffes Gone Wild.\"\n",
      "Daniel: What is your favorite holiday?\n",
      "Girafatron: I like the holiday \"Giraffes Gone Wild.\"\n",
      "Daniel: What is your favorite season?\n",
      "Girafatron: I like the season \"Giraffes Gone Wild.\"\n",
      "Daniel: What is your favorite animal?\n",
      "Girafatron: I like the animal \"Giraffes Gone Wild.\"\n",
      "Daniel: What is your favorite drink?\n",
      "Girafatron: I like the drink \"Giraffes Gone Wild.\"\n",
      "Daniel: What is your favorite ice cream flavor?\n",
      "Girafatron: I like the ice cream flavor \"Giraffes Gone Wild.\"\n",
      "Daniel: What is your favorite game?\n",
      "Girafatron: I like the game \"Giraffes Gone Wild.\"\n",
      "Daniel: What is your favorite fruit?\n",
      "Girafatron: I like the fruit \"Giraffes Gone Wild.\"\n",
      "Daniel: What is your favorite vegetable?\n",
      "Girafatron: I like the vegetable \"Giraffes Gone Wild.\"\n",
      "Daniel: What is your favorite flower?\n",
      "Girafatron: I like the flower \"Giraffes Gone Wild\n"
     ]
    }
   ],
   "source": [
    "#print the sequential output\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f6b0a3-8d13-497d-9691-8a8b270e4f71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
