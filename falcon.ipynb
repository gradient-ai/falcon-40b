{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be58009e-8307-40f2-9d0f-d1158c50ed8f",
   "metadata": {},
   "source": [
    "# Install the necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1e35dea-a981-414b-901b-a298f254f2bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T08:29:09.501666Z",
     "iopub.status.busy": "2023-11-16T08:29:09.501362Z",
     "iopub.status.idle": "2023-11-16T08:30:51.134619Z",
     "shell.execute_reply": "2023-11-16T08:30:51.134087Z",
     "shell.execute_reply.started": "2023-11-16T08:29:09.501663Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.13.1+cu116 requires torch==1.12.1, but you have torch 2.1.0 which is incompatible.\n",
      "torchaudio 0.12.1+cu116 requires torch==1.12.1, but you have torch 2.1.0 which is incompatible.\n",
      "gradient 2.0.6 requires marshmallow<3.0, but you have marshmallow 3.20.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#install the dependencies\n",
    "!pip install -qU transformers accelerate einops langchain xformers bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18ddc65-c88c-4b6a-bce7-402fefd57f66",
   "metadata": {},
   "source": [
    "# Login to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fe2dab0-e739-44bd-83ea-66a499abd85e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T11:07:44.817420Z",
     "iopub.status.busy": "2023-11-15T11:07:44.816789Z",
     "iopub.status.idle": "2023-11-15T11:07:44.835911Z",
     "shell.execute_reply": "2023-11-15T11:07:44.835206Z",
     "shell.execute_reply.started": "2023-11-15T11:07:44.817390Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a67b4e44d2f1484bada79450e101ca46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from huggingface_hub import login\n",
    "# login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f100fd-925b-4269-a7b3-ea03f86219a1",
   "metadata": {},
   "source": [
    "# Initialize and load a pre-trained model for language modeling using the Hugging Face Transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f99faa6e-62d6-4f3e-b90c-d03bc1058985",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T08:31:02.859802Z",
     "iopub.status.busy": "2023-11-16T08:31:02.859138Z",
     "iopub.status.idle": "2023-11-16T09:09:58.922419Z",
     "shell.execute_reply": "2023-11-16T09:09:58.921923Z",
     "shell.execute_reply.started": "2023-11-16T08:31:02.859772Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e916df24cc134c78b834612f4e0219c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)iuae/falcon-40b/resolve/main/config.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65b783abb6cf4d44a9e98f1239e07216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)40b/resolve/main/configuration_falcon.py:   0%|          | 0.00/7.16k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-40b:\n",
      "- configuration_falcon.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "\n",
      "WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ab7dd5487ed4fc69af1918200c8782d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)lcon-40b/resolve/main/modeling_falcon.py:   0%|          | 0.00/56.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-40b:\n",
      "- modeling_falcon.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0893d214a08a40538faa8e164f5dceb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)esolve/main/pytorch_model.bin.index.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a7049bdc7c64965b9355282106636f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3293bfbb40748549705bc5d45168e24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00009.bin:   0%|          | 0.00/9.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bda8881b17fa49cda6a44132b62ac3bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00009.bin:   0%|          | 0.00/9.51G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b4818dacba74a1ab2258a6224bbda75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00003-of-00009.bin:   0%|          | 0.00/9.51G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecc8fcfbbeb8414cb35d09737dd3823b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00004-of-00009.bin:   0%|          | 0.00/9.51G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca4f9b0943ed40c59eead39e62555123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00005-of-00009.bin:   0%|          | 0.00/9.51G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcbb6d1144384a95a72028b4872e3167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00006-of-00009.bin:   0%|          | 0.00/9.51G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6200b37e32384cdd9c4aad81c396984b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00007-of-00009.bin:   0%|          | 0.00/9.51G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64202c5f47ab4c0d960033670ced69fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00008-of-00009.bin:   0%|          | 0.00/9.51G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "891c1133c95c4b0a8b34299892a6861c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00009-of-00009.bin:   0%|          | 0.00/7.58G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21b5fde63c614b07ac28994eb7d2bd12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd5530c4e505413bbe3e4e5007c391ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-40b/resolve/main/generation_config.json:   0%|          | 0.00/117 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "#import necessary libraries to get the model running\n",
    "import torch\n",
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "\n",
    "model_name = 'tiiuae/falcon-40b'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# set quantization configuration to load large model with less GPU memory\n",
    "# this requires the `bitsandbytes` library\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto'\n",
    ")\n",
    "model.eval()\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8558c7-2c12-4902-a58e-77a53ddc237d",
   "metadata": {},
   "source": [
    "# Create the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8520da26-b608-4909-9980-2a9c5e896b29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T11:28:26.802794Z",
     "iopub.status.busy": "2023-11-16T11:28:26.802495Z",
     "iopub.status.idle": "2023-11-16T11:28:26.955959Z",
     "shell.execute_reply": "2023-11-16T11:28:26.955227Z",
     "shell.execute_reply.started": "2023-11-16T11:28:26.802773Z"
    }
   },
   "outputs": [],
   "source": [
    "#create a tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267096a1-c7c3-41a1-ab65-ba683055e0be",
   "metadata": {},
   "source": [
    "# Convert specific token sequences into their numerical representations a.k.a (token IDs) using a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fe992fe-8d9c-4c27-a01f-cb0fcfab2e82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T11:24:45.191653Z",
     "iopub.status.busy": "2023-11-16T11:24:45.190962Z",
     "iopub.status.idle": "2023-11-16T11:24:45.196249Z",
     "shell.execute_reply": "2023-11-16T11:24:45.195769Z",
     "shell.execute_reply.started": "2023-11-16T11:24:45.191653Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[23431, 37], [17362, 37]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "#convert token sequences into their corresponding token IDs\n",
    "stop_token_ids = [tokenizer.convert_tokens_to_ids(x) for x in [['Human', ':'], ['AI', ':']]]\n",
    "stop_token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b05cd1-b2a0-4918-a0e7-ed74305caf46",
   "metadata": {},
   "source": [
    "# Convert each integer value in the stop_token_ids list into a PyTorch LongTensor and then move the resulting tensors to a specified device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b471c840-030c-4645-84b9-3cccd3860e63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T11:24:48.507423Z",
     "iopub.status.busy": "2023-11-16T11:24:48.506782Z",
     "iopub.status.idle": "2023-11-16T11:24:48.523275Z",
     "shell.execute_reply": "2023-11-16T11:24:48.522505Z",
     "shell.execute_reply.started": "2023-11-16T11:24:48.507399Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([23431,    37], device='cuda:0'),\n",
       " tensor([17362,    37], device='cuda:0')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "#converting each integer value in stop_token_ids into a PyTorch LongTensor and moving it to a specified device\n",
    "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
    "stop_token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e41f5f-8f80-4acb-847f-d7822e33e3bb",
   "metadata": {},
   "source": [
    "# Define the Stopping criteria for Falcon 40b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca9b813f-cdc5-4b10-8576-cf01912d4ec9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T11:24:53.803629Z",
     "iopub.status.busy": "2023-11-16T11:24:53.803019Z",
     "iopub.status.idle": "2023-11-16T11:24:53.807960Z",
     "shell.execute_reply": "2023-11-16T11:24:53.807327Z",
     "shell.execute_reply.started": "2023-11-16T11:24:53.803577Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "# a custom stopping criteria object\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_ids in stop_token_ids:\n",
    "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d258000e-30c1-4fc2-b123-182303489ca0",
   "metadata": {},
   "source": [
    "# Set up a the text generation pipeline using the Hugging Face Transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d83737e-b10b-46dc-adb1-f89f640f3f52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T11:24:58.637969Z",
     "iopub.status.busy": "2023-11-16T11:24:58.637626Z",
     "iopub.status.idle": "2023-11-16T11:25:01.346561Z",
     "shell.execute_reply": "2023-11-16T11:25:01.345969Z",
     "shell.execute_reply.started": "2023-11-16T11:24:58.637939Z"
    }
   },
   "outputs": [],
   "source": [
    "#create the model pipeline\n",
    "generate_text = transformers.pipeline(\n",
    "    model=model, #pass the model\n",
    "    tokenizer=tokenizer, #pass the tokenizer\n",
    "    return_full_text=True,  #to return the original query, making it easier for prompting.\n",
    "    task='text-generation', #task\n",
    "    # we pass model parameters here too\n",
    "    stopping_criteria=stopping_criteria,  #to eliminate unnecessary conversations\n",
    "    temperature=0.3,  #for 'randomness' of model outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens=512,  #max number of tokens to generate in the output\n",
    "    repetition_penalty=1.1  #without this output begins repeating (make sure to experiment with this)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d96bbc5-b0ac-49ae-88f4-48c80f5f6bc6",
   "metadata": {},
   "source": [
    "# Generate the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de207d65-5d10-4c05-a173-839e3c2a4d40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T11:25:09.115909Z",
     "iopub.status.busy": "2023-11-16T11:25:09.115432Z",
     "iopub.status.idle": "2023-11-16T11:25:25.124065Z",
     "shell.execute_reply": "2023-11-16T11:25:25.123370Z",
     "shell.execute_reply.started": "2023-11-16T11:25:09.115886Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain to me the difference between centrifugal force and centripetal force.\n",
      "Centrifugal force is a fictitious force that acts on an object moving in a circular path. It is directed away from the center of rotation. Centripetal force is the real force that acts on an object moving in a circular path. It is directed toward the center of rotation.\n",
      "This page was last updated June 27, 2015.\n"
     ]
    }
   ],
   "source": [
    "#generate output\n",
    "res = generate_text(\"Explain to me the difference between centrifugal force and centripetal force.\")\n",
    "print(res[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c26515ed-0ffe-449d-9104-a4c24d775522",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T11:25:35.595282Z",
     "iopub.status.busy": "2023-11-16T11:25:35.594988Z",
     "iopub.status.idle": "2023-11-16T11:27:08.843915Z",
     "shell.execute_reply": "2023-11-16T11:27:08.843247Z",
     "shell.execute_reply.started": "2023-11-16T11:25:35.595262Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    }
   ],
   "source": [
    "#generate sequential output\n",
    "sequences = generate_text(\"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n",
    "    max_length=200,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93313606-b397-41e9-8ad0-f1f0a4920feb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T11:27:12.630980Z",
     "iopub.status.busy": "2023-11-16T11:27:12.630654Z",
     "iopub.status.idle": "2023-11-16T11:27:12.635234Z",
     "shell.execute_reply": "2023-11-16T11:27:12.634526Z",
     "shell.execute_reply.started": "2023-11-16T11:27:12.630955Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\n",
      "Daniel: Hello, Girafatron!\n",
      "Girafatron: Hello, Daniel.\n",
      "Daniel: How are you today?\n",
      "Girafatron: I am good.\n",
      "Daniel: That's good.\n",
      "Girafatron: Yes.\n",
      "Daniel: I'm glad to hear that.\n",
      "Girafatron: Thank you.\n",
      "Daniel: What are you doing today?\n",
      "Girafatron: I am going to the zoo.\n",
      "Daniel: Oh, that's nice.\n",
      "Girafatron: Yes.\n",
      "Daniel: Are you going to see the giraffes?\n",
      "Girafatron: Yes.\n",
      "Daniel: That's nice.\n",
      "Girafatron: Yes.\n",
      "Daniel: Well, I hope you have a good time.\n",
      "Girafatron: Thank you.\n",
      "Daniel: You're welcome.\n",
      "Girafatron: I will.\n",
      "Daniel: Good.\n",
      "Girafatron: Thank you.\n",
      "Daniel: You're welcome.\n",
      "Girafatron: I will.\n",
      "Daniel: Good.\n",
      "Girafatron: Thank you.\n",
      "Daniel: You're welcome.\n",
      "Girafatron: I will.\n",
      "Daniel: Good.\n",
      "Girafatron: Thank you.\n",
      "Daniel: You're welcome.\n",
      "Girafatron: I will.\n",
      "Daniel: Good.\n",
      "Girafatron: Thank you.\n",
      "Daniel: You're welcome.\n",
      "Girafatron: I will.\n",
      "Daniel: Good.\n",
      "Girafatron: Thank you.\n",
      "Daniel: You're welcome.\n",
      "Girafatron: I will.\n",
      "Daniel: Good.\n",
      "Girafatron: Thank you.\n",
      "Daniel: You're welcome.\n",
      "Girafatron: I will.\n",
      "Daniel: Good.\n",
      "Girafatron: Thank you.\n",
      "Daniel: You're welcome.\n",
      "Girafatron: I will.\n",
      "Daniel: Good.\n",
      "Girafatron: Thank you.\n",
      "Daniel: You're welcome.\n",
      "Girafatron: I will.\n",
      "Daniel: Good.\n",
      "Girafatron: Thank you.\n",
      "Daniel: You're welcome.\n",
      "Girafatron: I will.\n",
      "Daniel: Good.\n",
      "Girafatron: Thank you.\n",
      "Daniel: You're welcome.\n",
      "Girafatron: I will\n"
     ]
    }
   ],
   "source": [
    "#print the sequential output\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f6b0a3-8d13-497d-9691-8a8b270e4f71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
